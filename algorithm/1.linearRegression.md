# Linear Regression

## Simple linear regression

Simple linear regression represent linear regression with only one independent variable. 

For example, we have variable ![equation](http://latex.codecogs.com/gif.latex?Y=[y_1,y_2,...,y_n]) dependent on variable ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]). We are trying to find weights ![equation](http://latex.codecogs.com/gif.latex?w) and bias ![equation](http://latex.codecogs.com/gif.latex?b) that can model the relationship between them using the following formula: ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=wx_i+b) for i range from 1 to n.
Here ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}) is the predicted value, obviously, any ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) can be used to predict the dependent variable. What linear regression do is to find the best ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) that can minimize some predefined loss function.

### Loss function
A simple words for loss function, a loss function is defined to measure how bad the prediction is compared to the real values. For example, we have a 10 to predict, then 12 will be a better prediction than 15 (something like this). Many kinds of loss function could be used, here I will only introduce the Ordinary Least Squares (OLS) which is defined as: 

![equation](http://latex.codecogs.com/gif.latex?L=\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

which also always used as:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

I will introduce other loss function in multiple linear regression.

### Solution for simple linear regression

Math part, it is ok to skip this part and just look at the last to see the conclusions.

We clearly see that the loss function is larger or equal to zero,

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2\ge0)

and the loss function can tend to infinity. So to minimize the loss function, we can make the derivative of the loss function equal to zero. Then we do partial derivative for ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) respectively.

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{w}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)x_i=0)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{b}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)=0)

Then we have:

(1): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i^2+b\sum_{i=1}^{n}x_i=\sum_{i=1}^{n}(x_iy_i))     

(2): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i+bn=\sum_{i=1}^{n}(y_i))     

Then let ![equation](http://latex.codecogs.com/gif.latex?(1)\times{n}-(2)\times{\sum_{i=1}^{n}x_i})

So then we have:

![equation](http://latex.codecogs.com/gif.latex?w=\frac{n\sum_{i=1}^{n}(x_iy_i)-(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)}{n\sum_{i=1}^{n}x_i^2-(\sum_{i=1}^nx_i)^2}) 

with the value of ![equation](http://latex.codecogs.com/gif.latex?w), we can easily get:

![equation](http://latex.codecogs.com/gif.latex?b=\frac{\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i}{n}) 

Finally with the two equations above, we solved the simple linear regression problem.

### Python solution from scratch

Belowing the the python codes solving the simple linear regression from scratch.

```python
import os, glob, math
import numpy as np

class linear_model:
  """
  linear model class, this is only for simple linear regression
  """
  def __init__(self):
    self.rmse = None
    pass
  
  def train(self, X, y):
    """
    @param X, independent variable, here requires the X to have only one variable.
    Example: X = np.array([1,2,3,4,...])
    @param y, dependent variable, example y = np.array([1,2,3,4,...])
    @return, this function will return weight paramether w and bias parameter b
    """
    n = len(X)
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input, this function only for simple linear regression
    X = np.array(X)
    y = np.array(y)
    if len(X.shape) > 1:
      X = X.reshape(n)
    if len(y.shape) > 1:
      y = y.reshape(n)
    
    # get the parameter w and b
    sumx = np.sum(X)
    sumy = np.sum(y)
    xy = X.dot(y)
    xsquare = X.dot(X)
    
    w = (n*xy - sumx*sumy) / (n*xsquare - sumx**2)
    b = (sumy - w * sumx) / n
    self.w = w
    self.b = b
    return w, b
  
  def predict(self, X, y=None):
    """
    @param X, independent variable, here requires the X to have only one variable.
    Example: X = np.array([1,2,3,4,...])
    @param y, dependent variable, example y = np.array([1,2,3,4,...])
    @return, return the predicted dependent variables for input X, or if y is specified, also calculate the RMSE (root mean square error)
    """
    n = len(X)
    X = np.array(X)
    if len(X.shape) > 1:
      X = X.reshape(n)
    y_pred = self.w * X + self.b
    self.y_pred = y_pred
    if y != None:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      if len(y.shape) > 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return y_pred
      
```

This is easy, and the code also can be find at ![simple linear regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/codes/simple_linear_regression.py)

## Multiple linear regression

This is again math part, just skip to the end of this part to check the results if you like.

After we have get the solution for simple linear regression, we will come to check the multiple linear regression. The difference is that, the independent variable could now have multiple variables, for ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]), each sample could be described by many parameters, ![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im}]), this is each sample have m variables.

Then for each dependent variable, we could model is with the independent variables as 

![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=\sum_{j=1}^{m}w_jx_{ij}+b=Wx_i^T+b)

where 

![equation](http://latex.codecogs.com/gif.latex?W=[w_1,w_2,...,w_m])

The simple linear regression is a specific case of multiple linear regression, so the following part of solution can also be used to do simple linear regression. And we use the Ordinary Least Squares (OLS): 

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

Similar as in the simple linear regression part, we could make the patial derivatives of the loss function to be zero. Here is how we process:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2=\frac{1}{n}\sum_{i=1}^{n}(y_i-Wx_i^T-b)^2)

Then we rewrite the parameters and independent variables of each sample, for example,

![equation](http://latex.codecogs.com/gif.latex?\theta=[w_1,w_2,...,w_m,b]^T)

and

![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im},1]^T)

both ![equation](http://latex.codecogs.com/gif.latex?\theta) and ![equation](http://latex.codecogs.com/gif.latex?x_i) are column vectors with size ![equation](http://latex.codecogs.com/gif.latex?(m+1)\times{(1)}).Then we can get the linear model with:

![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=\sum_{j=1}^{m}w_jx_{ij}+b={x_i}^T\theta)

Combine the dependent variable ![equation](http://latex.codecogs.com/gif.latex?y_i) into column vector:

![equation](http://latex.codecogs.com/gif.latex?\overrightarrow{y}=[y_1,y_2,...,y_n]^T)

And also stack the independent variable into column vectors:

![equation](http://latex.codecogs.com/gif.latex?X=[x_1^T,x_2^T,...,x_{m+1}^T])

The size of ![equation](http://latex.codecogs.com/gif.latex?\overrightarrow{y}) and ![equation](http://latex.codecogs.com/gif.latex?X) is ![equation](http://latex.codecogs.com/gif.latex?n\times1) and ![equation](http://latex.codecogs.com/gif.latex?n\times(m+1)), respectively.

Then we re-formula the loss function as

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-Wx_i^T-b)^2=\frac{1}{n}({x_i}^T\theta-y_i)^2=\frac{1}{n}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}))

This is similar to the following loss function (only divided a constant number):

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=\frac{1}{2}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}))

Then we calculate the derivatives of the loss function to every parameter as a parameter vector, which is also called gradient. The gradient of the loss function is calculated as following:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}J(\theta)=\nabla_{\theta}\frac{1}{2}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}))

![equation](http://latex.codecogs.com/gif.latex?=\frac{1}{2}\nabla_{\theta}(\theta^TX^TX\theta-\theta^TX^T\overrightarrow{y}-\overrightarrow{y}^TX\theta+\overrightarrow{y}^T\overrightarrow{y}))

![equation](http://latex.codecogs.com/gif.latex?=\frac{1}{2}\nabla_{\theta}(\theta^TX^TX\theta-\theta^TX^T\overrightarrow{y}-\overrightarrow{y}^TX\theta))

![equation](http://latex.codecogs.com/gif.latex?=\frac{1}{2}\nabla_{\theta}tr(\theta^TX^TX\theta-\theta^TX^T\overrightarrow{y}-\overrightarrow{y}^TX\theta))

![equation](http://latex.codecogs.com/gif.latex?=\frac{1}{2}\nabla_{\theta}(tr\theta^TX^TX\theta-2tr\overrightarrow{y}^TX\theta))

![equation](http://latex.codecogs.com/gif.latex?=\frac{1}{2}(X^TX\theta+X^TX\theta-2X^T\overrightarrow{y}))

![equation](http://latex.codecogs.com/gif.latex?=X^TX\theta-X^T\overrightarrow{y})

When the gradient is zero, we will have the optimal parameters for the multiple linear regression under the OLS loss function, so we have:

![equation](http://latex.codecogs.com/gif.latex?X^TX\theta-X^T\overrightarrow{y}=0)

Then 

![equation](http://latex.codecogs.com/gif.latex?X^TX\theta=X^T\overrightarrow{y})

So finally we get:

![equation](http://latex.codecogs.com/gif.latex?\theta=(X^TX)^{-1}X^T\overrightarrow{y})

So after all this we get the solution for multiple linear regression under OLS loss function.

## Python solution from scratch for multiple linear regression

This will cover the simple linear regression, that is simple linear regression can also be train / predict using following model.

```python
import os, glob, math
import numpy as np

class linear_model:
  """
  linear model class, this is linear regression
  """
  def __init__(self):
    self.rmse = None
    pass
  
  def train(self, X, y):
    """
    If data not the required format, this function not works
    @param X, independent variable, example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    Both X and y should be an array.
    @return, this function will return weight vector w and bias parameter b
    """
    n = len(X)
    m = len(X[0])
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input, this function only for simple linear regression
    X = np.array(X)
    y = np.array(y)
    
    if len(y.shape) != 2:
      y = y.reshape(n,1)
    
    # add a 1 column for the bias variable
    X = np.column_stack((X, np.ones(n)))
    
    # get the parameter w and b
    theta=np.array(np.mat(X.T.dot(X)).I.dot(X.T).dot(y))
    self.w = theta[:-1]
    self.b = theta[-1]
    return theta
  
  def predict(self, X, y=None):
    """
    @param X, independent variable.
    Example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    @return, return the predicted dependent variables for input X, or if y is specified, also calculate the RMSE (root mean square error)
    """
    n = len(X)
    X = np.array(X)
    y_pred = X.dot(self.w) + self.b
    self.y_pred = y_pred
    if y != None:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      y_pred = y_pred.reshape(n)
      if len(y.shape) != 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return self.y_pred
      
```

After all we modelled the linear regression from scratch, using only numerical calculation in Numpy.

Then we will discuss about Gradient Descent algorithms to solve arbitrary loss function, such as L1-norm for linear regression, cross entropy for logistic regression.



